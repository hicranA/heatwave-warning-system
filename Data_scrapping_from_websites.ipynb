{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0294fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import csv\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_researchgate(query):\n",
    "  \"\"\"Scrapes the ResearchGate website for papers and journals that contain the given query.\n",
    "\n",
    "  Args:\n",
    "    query: The query to search for.\n",
    "\n",
    "  Returns:\n",
    "    A list of dictionaries, where each dictionary contains information about a\n",
    "    paper or journal, including the title, the name of the author, the publication\n",
    "    date, the citations, and the link.\n",
    "  \"\"\"\n",
    "\n",
    "  # Create a new Selenium webdriver instance.\n",
    "  driver = webdriver.Chrome()\n",
    "\n",
    "  try:\n",
    "    # Navigate to the ResearchGate website.\n",
    "    driver.get(\"https://www.researchgate.net/\")\n",
    "\n",
    "    # Wait for the search bar to be visible.\n",
    "    search_bar = WebDriverWait(driver, 60).until(\n",
    "        EC.visibility_of_element_located((By.CSS_SELECTOR, \"input[aria-label='Search ResearchGate']\"))\n",
    "    )\n",
    "\n",
    "    # Enter the query in the search bar.\n",
    "    search_bar.send_keys(query)\n",
    "\n",
    "    # Find the search button and click it.\n",
    "    search_button = driver.find_element(By.CSS_SELECTOR, \"button#searchButton\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Wait for the search results to load.\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \".result\"))\n",
    "    )\n",
    "\n",
    "    # Get all of the paper and journal results.\n",
    "    results = driver.find_elements(By.CSS_SELECTOR, \".result\")\n",
    "\n",
    "    # Create a list to store the scraped data.\n",
    "    scraped_data = []\n",
    "\n",
    "    # Iterate over the results and scrape the data.\n",
    "    for result in results:\n",
    "      title = result.find_element(By.CSS_SELECTOR, \"h2.title\").text\n",
    "      author = result.find_element(By.CSS_SELECTOR, \".author\").text\n",
    "      publication_date = result.find_element(By.CSS_SELECTOR, \".publication-date\").text\n",
    "      citations = result.find_element(By.CSS_SELECTOR, \".citations\").text\n",
    "      link = result.find_element(By.CSS_SELECTOR, \".link\").get_attribute(\"href\")\n",
    "\n",
    "      # Create a dictionary to store the scraped data.\n",
    "      data = {\n",
    "          \"title\": title,\n",
    "          \"author\": author,\n",
    "          \"publication_date\": publication_date,\n",
    "          \"citations\": citations,\n",
    "          \"link\": link\n",
    "      }\n",
    "\n",
    "      # Add the dictionary to the scraped data list.\n",
    "      scraped_data.append(data)\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "  finally:\n",
    "    # Close the Selenium webdriver instance.\n",
    "    driver.quit()\n",
    "\n",
    "# Scrape the ResearchGate website for papers and journals that contain the words 'heat wave'.\n",
    "scraped_data = scrape_researchgate(\"heat wave\")\n",
    "\n",
    "# Save the scraped data to a CSV file.\n",
    "with open(\"researchgate_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "  writer = csv.DictWriter(csvfile, fieldnames=[\"title\", \"author\", \"publication_date\", \"citations\", \"link\"])\n",
    "  writer.writeheader()\n",
    "  writer.writerows(scraped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f481e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be84e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_researchgate(query):\n",
    "  \"\"\"Scrapes the ResearchGate website for papers and journals that contain the given query.\n",
    "\n",
    "  Args:\n",
    "    query: The query to search for.\n",
    "\n",
    "  Returns:\n",
    "    A list of dictionaries, where each dictionary contains information about a\n",
    "    paper or journal, including the title, the name of the author, the publication\n",
    "    date, the citations, and the link.\n",
    "  \"\"\"\n",
    "\n",
    "  # Make a request to the ResearchGate website.\n",
    "  response = requests.get(\"https://www.researchgate.net/search\")\n",
    "\n",
    "  # Parse the HTML response.\n",
    "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "  # Extract the data from the HTML.\n",
    "  data = []\n",
    "\n",
    "  # Find all of the search results.\n",
    "  search_results = soup.find_all(\"div\", class_=\"result\")\n",
    "\n",
    "  # Iterate over the search results and extract the data.\n",
    "  for search_result in search_results:\n",
    "    title = search_result.find(\"h2\", class_=\"title\").text\n",
    "    author = search_result.find(\"a\", class_=\"author\").text\n",
    "    publication_date = search_result.find(\"span\", class_=\"publication-date\").text\n",
    "    citations = search_result.find(\"span\", class_=\"citations\").text\n",
    "    link = search_result.find(\"a\", class_=\"link\").get(\"href\")\n",
    "\n",
    "    # Create a dictionary to store the scraped data.\n",
    "    data.append({\n",
    "      \"title\": title,\n",
    "      \"author\": author,\n",
    "      \"publication_date\": publication_date,\n",
    "      \"citations\": citations,\n",
    "      \"link\": link\n",
    "    })\n",
    "\n",
    "  return data\n",
    "\n",
    "# Scrape the ResearchGate website and save the data to a CSV file.\n",
    "with open(\"researchgate_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "  writer = csv.DictWriter(csvfile, fieldnames=[\"title\", \"author\", \"publication_date\", \"citations\", \"link\"])\n",
    "  writer.writeheader()\n",
    "\n",
    "  for data in scrape_researchgate(\"heat wave\"):\n",
    "    writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d827050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_google_scholar(query):\n",
    "  \"\"\"Scrapes the Google Scholar website for papers and journals that contain the given query.\n",
    "\n",
    "  Args:\n",
    "    query: The query to search for.\n",
    "\n",
    "  Returns:\n",
    "    A list of dictionaries, where each dictionary contains information about a\n",
    "    paper or journal, including the title, the name of the author, the publication\n",
    "    date, the citations, and the link.\n",
    "  \"\"\"\n",
    "\n",
    "  # Make a request to the Google Scholar website.\n",
    "  response = requests.get(\"https://scholar.google.com/scholar?hl=en&q=\" + query)\n",
    "\n",
    "  # Parse the HTML response.\n",
    "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "  # Extract the data from the HTML.\n",
    "  data = []\n",
    "\n",
    "  # Find all of the search results.\n",
    "  search_results = soup.find_all(\"div\", class_=\"gs_r\")\n",
    "\n",
    "  # Iterate over the search results and extract the data.\n",
    "  for search_result in search_results:\n",
    "    try:\n",
    "      title = search_result.find(\"h3\", class_=\"gs_rt\").text\n",
    "    except AttributeError:\n",
    "      title = \"\"\n",
    "\n",
    "    author = search_result.find(\"span\", class_=\"gs_ai_name\")\n",
    "    if author is not None:\n",
    "      author = author.text\n",
    "    else:\n",
    "      author = \"\"\n",
    "\n",
    "    try:\n",
    "      publication_date = search_result.find(\"div\", class_=\"gs_a\").text\n",
    "    except AttributeError:\n",
    "      publication_date = \"\"\n",
    "\n",
    "    try:\n",
    "      citations = search_result.find(\"div\", class_=\"gs_ai_c\").text\n",
    "    except AttributeError:\n",
    "      citations = \"\"\n",
    "\n",
    "    try:\n",
    "      link = search_result.find(\"a\", class_=\"gs_ai_u\").get(\"href\")\n",
    "    except AttributeError:\n",
    "      link = \"\"\n",
    "\n",
    "    # Create a dictionary to store the scraped data.\n",
    "    data.append({\n",
    "      \"title\": title,\n",
    "      \"author\": author,\n",
    "      \"publication_date\": publication_date,\n",
    "      \"citations\": citations,\n",
    "      \"link\": link\n",
    "    })\n",
    "\n",
    "  return data\n",
    "\n",
    "# Scrape Google Scholar and save the data to a CSV file.\n",
    "with open(\"google_scholar_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "  writer = csv.DictWriter(csvfile, fieldnames=[\"title\", \"author\", \"publication_date\", \"citations\", \"link\"])\n",
    "  writer.writeheader()\n",
    "\n",
    "  for data in scrape_google_scholar(\"heatwave\"):\n",
    "    writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9de97282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_google_scholar(query):\n",
    "  \"\"\"Scrapes the Google Scholar website for papers and journals that contain the given query.\n",
    "\n",
    "  Args:\n",
    "    query: The query to search for.\n",
    "\n",
    "  Returns:\n",
    "    A list of dictionaries, where each dictionary contains information about a\n",
    "    paper or journal, including the title, the name of the author, the publication\n",
    "    date, the citations, and the link.\n",
    "  \"\"\"\n",
    "\n",
    "  # Make a request to the Google Scholar website.\n",
    "  response = requests.get(\"https://scholar.google.com/scholar?hl=en&q=\" + query)\n",
    "\n",
    "  # Parse the HTML response.\n",
    "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "  # Extract the data from the HTML.\n",
    "  data = []\n",
    "\n",
    "  # Find all of the search results.\n",
    "  search_results = soup.find_all(\"div\", class_=\"gs_r\")\n",
    "\n",
    "  # Iterate over the search results and extract the data.\n",
    "  for search_result in search_results:\n",
    "    try:\n",
    "      title = search_result.find(\"h3\", class_=\"gs_rt\").text\n",
    "    except AttributeError:\n",
    "      title = \"\"\n",
    "\n",
    "    try:\n",
    "      author = search_result.find(\"span\", class_=\"gs_ai_name\").text\n",
    "    except AttributeError:\n",
    "      author = \"\"\n",
    "\n",
    "    try:\n",
    "      publication_date = search_result.find(\"div\", class_=\"gs_a\").text\n",
    "    except AttributeError:\n",
    "      publication_date = \"\"\n",
    "\n",
    "    try:\n",
    "      citations = search_result.find(\"div\", class_=\"gs_ai_c\").text\n",
    "    except AttributeError:\n",
    "      citations = \"\"\n",
    "\n",
    "    try:\n",
    "      link = search_result.find(\"a\", class_=\"gs_ai_u\").get(\"href\")\n",
    "    except AttributeError:\n",
    "      link = \"\"\n",
    "\n",
    "    # Create a dictionary to store the scraped data.\n",
    "    data.append({\n",
    "      \"title\": title,\n",
    "      \"author\": author,\n",
    "      \"publication_date\": publication_date,\n",
    "      \"citations\": citations,\n",
    "      \"link\": link\n",
    "    })\n",
    "\n",
    "  return data\n",
    "\n",
    "# Scrape Google Scholar and save the data to a CSV file.\n",
    "with open(\"google_scholar_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "  writer = csv.DictWriter(csvfile, fieldnames=[\"title\", \"author\", \"publication_date\", \"citations\", \"link\"])\n",
    "  writer.writeheader()\n",
    "\n",
    "  for data in scrape_google_scholar(\"heatwave\"):\n",
    "    # Write the empty strings to the CSV file if the author, citations, and link elements are not present.\n",
    "    if data[\"author\"] == \"\":\n",
    "      data[\"author\"] = \"None\"\n",
    "    if data[\"citations\"] == \"\":\n",
    "      data[\"citations\"] = \"None\"\n",
    "    if data[\"link\"] == \"\":\n",
    "      data[\"link\"] = \"None\"\n",
    "\n",
    "    writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "741308c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_google_scholar(query):\n",
    "  \"\"\"Scrapes the Google Scholar website for papers and journals that contain the given query.\n",
    "\n",
    "  Args:\n",
    "    query: The query to search for.\n",
    "\n",
    "  Returns:\n",
    "    A list of dictionaries, where each dictionary contains information about a\n",
    "    paper or journal, including the title, the name of the author, the publication\n",
    "    date, the citations, and the link.\n",
    "  \"\"\"\n",
    "\n",
    "  # Make a request to the Google Scholar website.\n",
    "  response = requests.get(\"https://scholar.google.com/scholar?hl=en&q=\" + query)\n",
    "\n",
    "  # Parse the HTML response.\n",
    "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "  # Extract the data from the HTML.\n",
    "  data = []\n",
    "\n",
    "  # Find all of the search results.\n",
    "  search_results = soup.find_all(\"div\", class_=\"gs_r\")\n",
    "\n",
    "  # Iterate over the search results and extract the data.\n",
    "  for search_result in search_results:\n",
    "    try:\n",
    "      title = search_result.find(\"h3\", class_=\"gs_rt\").text\n",
    "    except AttributeError:\n",
    "      title = \"\"\n",
    "\n",
    "    try:\n",
    "      author = search_result.find(\"span\", class_=\"gs_ai_name\").text\n",
    "    except AttributeError:\n",
    "      author = \"\"\n",
    "\n",
    "    try:\n",
    "      publication_date = search_result.find(\"div\", class_=\"gs_a\").text\n",
    "    except AttributeError:\n",
    "      publication_date = \"\"\n",
    "\n",
    "    try:\n",
    "      citations = search_result.find(\"div\", class_=\"gs_ai_c\").text\n",
    "    except AttributeError:\n",
    "      citations = \"\"\n",
    "\n",
    "    try:\n",
    "      # Get the URL of the paper.\n",
    "      link = search_result.find(\"a\", class_=\"gs_ai_u\").get(\"href\")\n",
    "    except AttributeError:\n",
    "      link = \"\"\n",
    "\n",
    "    # Create a dictionary to store the scraped data.\n",
    "    data.append({\n",
    "      \"title\": title,\n",
    "      \"author\": author,\n",
    "      \"publication_date\": publication_date,\n",
    "      \"citations\": citations,\n",
    "      \"link\": link\n",
    "    })\n",
    "\n",
    "  return data\n",
    "\n",
    "# Scrape Google Scholar and save the data to a CSV file.\n",
    "with open(\"google_scholar_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "  writer = csv.DictWriter(csvfile, fieldnames=[\"title\", \"author\", \"publication_date\", \"citations\", \"link\"])\n",
    "  writer.writeheader()\n",
    "\n",
    "  for data in scrape_google_scholar(\"heatwave\"):\n",
    "    # Write the empty strings to the CSV file if the author, citations, and link elements are not present.\n",
    "    if data[\"author\"] == \"\":\n",
    "      data[\"author\"] = \"None\"\n",
    "    if data[\"citations\"] == \"\":\n",
    "      data[\"citations\"] = \"None\"\n",
    "    if data[\"link\"] == \"\":\n",
    "      data[\"link\"] = \"None\"\n",
    "\n",
    "    writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a0dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_google_scholar(query):\n",
    "    \"\"\"Scrapes the Google Scholar website for papers and journals that contain the given query.\n",
    "\n",
    "    Args:\n",
    "        query: The query to search for.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary contains information about a\n",
    "        paper or journal, including the title, the name of the author, the publication\n",
    "        date, the citations, and the link.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a request to the Google Scholar website.\n",
    "    response = requests.get(\"https://scholar.google.com/scholar?hl=en&q=\" + query)\n",
    "\n",
    "    # Parse the HTML response.\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract the data from the HTML.\n",
    "    data = []\n",
    "\n",
    "    # Find all of the search results.\n",
    "    search_results = soup.find_all(\"div\", class_=\"gs_r\")\n",
    "\n",
    "    # Iterate over the search results and extract the data.\n",
    "    for search_result in search_results:\n",
    "        try:\n",
    "            title = search_result.find(\"h3\", class_=\"gs_rt\").text\n",
    "        except AttributeError:\n",
    "            title = \"\"\n",
    "\n",
    "        try:\n",
    "            author = search_result.find(\"div\", class_=\"gs_a\").text\n",
    "        except AttributeError:\n",
    "            author = \"None\"\n",
    "\n",
    "        try:\n",
    "            # Get the URL of the paper.\n",
    "            link = search_result.find(\"a\", class_=\"gs_or_ggsm\").get(\"href\")\n",
    "        except AttributeError:\n",
    "            link = \"None\"\n",
    "\n",
    "        try:\n",
    "            citations = search_result.find(\"div\", class_=\"gs_fl\").text\n",
    "        except AttributeError:\n",
    "            citations = \"None\"\n",
    "\n",
    "        # Create a dictionary to store the scraped data.\n",
    "        data.append({\n",
    "            \"title\": title,\n",
    "            \"author\": author,\n",
    "            \"citations\": citations,\n",
    "            \"link\": link\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape Google Scholar and save the data to a CSV file.\n",
    "with open(\"google_scholar_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"title\", \"author\", \"citations\", \"link\"])\n",
    "    writer.writeheader()\n",
    "\n",
    "    for data in scrape_google_scholar(\"heatwave\"):\n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98dde4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_google_scholar(query):\n",
    "    \"\"\"Scrapes the Google Scholar website for papers and journals that contain the given query.\n",
    "\n",
    "    Args:\n",
    "        query: The query to search for.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary contains information about a\n",
    "        paper or journal, including the title, the name of the author, the publication\n",
    "        date, the citations, and the link.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a request to the Google Scholar website.\n",
    "    response = requests.get(\"https://scholar.google.com/scholar?hl=en&q=\" + query)\n",
    "\n",
    "    # Parse the HTML response.\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract the data from the HTML.\n",
    "    data = []\n",
    "\n",
    "    # Find all of the search results.\n",
    "    search_results = soup.find_all(\"div\", class_=\"gs_r\")\n",
    "\n",
    "    # Iterate over the search results and extract the data.\n",
    "    for search_result in search_results:\n",
    "        try:\n",
    "            title = search_result.find(\"h3\", class_=\"gs_rt\").text\n",
    "        except AttributeError:\n",
    "            title = \"\"\n",
    "\n",
    "        try:\n",
    "            author = search_result.find(\"div\", class_=\"gs_a\").text\n",
    "        except AttributeError:\n",
    "            author = \"None\"\n",
    "\n",
    "        try:\n",
    "            # Get the URL of the paper.\n",
    "            link = search_result.find(\"a\", class_=\"gs_or_ggsm\").get(\"href\")\n",
    "        except AttributeError:\n",
    "            link = \"None\"\n",
    "\n",
    "        try:\n",
    "            citations = search_result.find(\"div\", class_=\"gs_fl\").text\n",
    "        except AttributeError:\n",
    "            citations = \"None\"\n",
    "\n",
    "        # Create a dictionary to store the scraped data.\n",
    "        data.append({\n",
    "            \"title\": title,\n",
    "            \"author\": author,\n",
    "            \"citations\": citations,\n",
    "            \"link\": link\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape Google Scholar and save the data to a CSV file.\n",
    "with open(\"google_scholar_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"title\", \"author\", \"citations\", \"link\"])\n",
    "    writer.writeheader()\n",
    "\n",
    "    for data in scrape_google_scholar(\"heatwave\"):\n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d63545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_google_scholar(query):\n",
    "    \"\"\"Scrapes the Google Scholar website for papers and journals that contain the given query.\n",
    "\n",
    "    Args:\n",
    "        query: The query to search for.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary contains information about a\n",
    "        paper or journal, including author(s), title, publication date, source, and page numbers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a request to the Google Scholar website.\n",
    "    response = requests.get(\"https://scholar.google.com/scholar?hl=en&q=\" + query)\n",
    "\n",
    "    # Parse the HTML response.\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract the data from the HTML.\n",
    "    data = []\n",
    "\n",
    "    # Find all of the search results.\n",
    "    search_results = soup.find_all(\"div\", class_=\"gs_r\")\n",
    "\n",
    "    # Iterate over the search results and extract the data.\n",
    "    for search_result in search_results:\n",
    "        try:\n",
    "            author = search_result.find(\"div\", class_=\"gs_a\").text\n",
    "        except AttributeError:\n",
    "            author = \"None\"\n",
    "\n",
    "        try:\n",
    "            title = search_result.find(\"h3\", class_=\"gs_rt\").text\n",
    "        except AttributeError:\n",
    "            title = \"None\"\n",
    "\n",
    "        try:\n",
    "            source = search_result.find(\"div\", class_=\"gs_a\").text\n",
    "        except AttributeError:\n",
    "            source = \"None\"\n",
    "\n",
    "        try:\n",
    "            publication_date = search_result.find(\"div\", class_=\"gs_a\").text\n",
    "        except AttributeError:\n",
    "            publication_date = \"None\"\n",
    "\n",
    "        try:\n",
    "            # Get the URL of the paper.\n",
    "            link = search_result.find(\"a\", class_=\"gs_or_ggsm\").get(\"href\")\n",
    "        except AttributeError:\n",
    "            link = \"None\"\n",
    "\n",
    "        try:\n",
    "            # Extract page numbers if available.\n",
    "            page_numbers = search_result.find(\"div\", class_=\"gs_fl\").text\n",
    "        except AttributeError:\n",
    "            page_numbers = \"None\"\n",
    "\n",
    "        # Create a dictionary to store the scraped data.\n",
    "        data.append({\n",
    "            \"Author(s)\": author,\n",
    "            \"Title\": title,\n",
    "            \"Publication Date\": publication_date,\n",
    "            \"Source\": source,\n",
    "            \"Page Numbers\": page_numbers,\n",
    "            \"Link\": link\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape Google Scholar and save the data to a CSV file.\n",
    "with open(\"google_scholar_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"Author(s)\", \"Title\", \"Publication Date\", \"Source\", \"Page Numbers\", \"Link\"])\n",
    "    writer.writeheader()\n",
    "\n",
    "    for data in scrape_google_scholar(\"heat waves\"):\n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e249d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_google_scholar(keywords):\n",
    "    \"\"\"Scrapes the Google Scholar website for papers and journals that contain the given keywords.\n",
    "\n",
    "    Args:\n",
    "        keywords: A list of keywords to search for.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary contains information about a\n",
    "        paper or journal, including author(s), title, publication date, source, and page numbers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine the keywords into a single search query.\n",
    "    query = \"+\".join(keywords)\n",
    "\n",
    "    # Make a request to the Google Scholar website.\n",
    "    response = requests.get(\"https://scholar.google.com/scholar?hl=en&q=\" + query)\n",
    "\n",
    "    # Parse the HTML response.\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract the data from the HTML.\n",
    "    data = []\n",
    "\n",
    "    # Find all of the search results.\n",
    "    search_results = soup.find_all(\"div\", class_=\"gs_r\")\n",
    "\n",
    "    # Iterate over the search results and extract the data.\n",
    "    for search_result in search_results:\n",
    "        try:\n",
    "            author = search_result.find(\"div\", class_=\"gs_a\").text\n",
    "        except AttributeError:\n",
    "            author = \"None\"\n",
    "\n",
    "        try:\n",
    "            title = search_result.find(\"h3\", class_=\"gs_rt\").text\n",
    "        except AttributeError:\n",
    "            title = \"None\"\n",
    "\n",
    "        try:\n",
    "            source = search_result.find(\"div\", class_=\"gs_a\").text\n",
    "        except AttributeError:\n",
    "            source = \"None\"\n",
    "\n",
    "        try:\n",
    "            publication_date = search_result.find(\"div\", class_=\"gs_a\").text\n",
    "        except AttributeError:\n",
    "            publication_date = \"None\"\n",
    "\n",
    "        try:\n",
    "            # Get the URL of the paper.\n",
    "            link = search_result.find(\"a\", class_=\"gs_or_ggsm\").get(\"href\")\n",
    "        except AttributeError:\n",
    "            link = \"None\"\n",
    "\n",
    "        try:\n",
    "            # Extract page numbers if available.\n",
    "            page_numbers = search_result.find(\"div\", class_=\"gs_fl\").text\n",
    "        except AttributeError:\n",
    "            page_numbers = \"None\"\n",
    "\n",
    "        # Create a dictionary to store the scraped data.\n",
    "        data.append({\n",
    "            \"Author(s)\": author,\n",
    "            \"Title\": title,\n",
    "            \"Publication Date\": publication_date,\n",
    "            \"Source\": source,\n",
    "            \"Page Numbers\": page_numbers,\n",
    "            \"Link\": link\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# List of keywords to search for.\n",
    "keywords = [\"heatwave\", \"heat wave\"]\n",
    "\n",
    "# Scrape Google Scholar and save the data to a CSV file.\n",
    "with open(\"google_scholar_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"Author(s)\", \"Title\", \"Publication Date\", \"Source\", \"Page Numbers\", \"Link\"])\n",
    "    writer.writeheader()\n",
    "\n",
    "    for keyword in keywords:\n",
    "        for data in scrape_google_scholar([keyword]):\n",
    "            writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a388ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_google_scholar(keywords):\n",
    "    \"\"\"Scrapes the Google Scholar website for papers and journals that contain the given keywords.\n",
    "\n",
    "    Args:\n",
    "        keywords: A list of keywords to search for.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary contains information about a\n",
    "        paper or journal, including author(s), title, publication date, source, and page numbers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine the keywords into a single search query.\n",
    "    query = \"+\".join(keywords)\n",
    "\n",
    "    # Make a request to the Google Scholar website.\n",
    "    response = requests.get(\"https://scholar.google.com/scholar?hl=en&q=\" + query)\n",
    "\n",
    "    # Parse the HTML response.\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract the data from the HTML.\n",
    "    data = []\n",
    "\n",
    "    # Find all of the search results.\n",
    "    search_results = soup.find_all(\"div\", class_=\"gs_r\")\n",
    "\n",
    "    # Iterate over the search results and extract the data.\n",
    "    for search_result in search_results:\n",
    "        try:\n",
    "            author = search_result.find(\"div\", class_=\"gs_a\").text\n",
    "        except AttributeError:\n",
    "            author = \"None\"\n",
    "\n",
    "        try:\n",
    "            title_link = search_result.find(\"h3\", class_=\"gs_rt\").find(\"a\")\n",
    "            title = title_link.text\n",
    "            # Get the URL of the paper from the link.\n",
    "            link = title_link.get(\"href\")\n",
    "        except AttributeError:\n",
    "            title = \"None\"\n",
    "            link = \"None\"\n",
    "\n",
    "        try:\n",
    "            source = search_result.find(\"div\", class_=\"gs_a\").text\n",
    "        except AttributeError:\n",
    "            source = \"None\"\n",
    "\n",
    "        try:\n",
    "            publication_date = search_result.find(\"div\", class_=\"gs_a\").text\n",
    "        except AttributeError:\n",
    "            publication_date = \"None\"\n",
    "\n",
    "        try:\n",
    "            # Extract page numbers if available.\n",
    "            page_numbers = search_result.find(\"div\", class_=\"gs_fl\").text\n",
    "        except AttributeError:\n",
    "            page_numbers = \"None\"\n",
    "\n",
    "        # Create a dictionary to store the scraped data.\n",
    "        data.append({\n",
    "            \"Author(s)\": author,\n",
    "            \"Title\": title,\n",
    "            \"Publication Date\": publication_date,\n",
    "            \"Source\": source,\n",
    "            \"Page Numbers\": page_numbers,\n",
    "            \"Link\": link\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# List of keywords to search for.\n",
    "keywords = [\"heatwave\", \"heat wave\"]\n",
    "\n",
    "# Scrape Google Scholar and save the data to a CSV file.\n",
    "with open(\"google_scholar_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"Author(s)\", \"Title\", \"Publication Date\", \"Source\", \"Page Numbers\", \"Link\"])\n",
    "    writer.writeheader()\n",
    "\n",
    "    for keyword in keywords:\n",
    "        for data in scrape_google_scholar([keyword]):\n",
    "            writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08160479",
   "metadata": {},
   "source": [
    "# This is the code i used eventually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56777fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_researchgate(query):\n",
    "    \"\"\"Scrapes ResearchGate for research papers and journals that contain the given query.\n",
    "\n",
    "    Args:\n",
    "        query: The query to search for.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary contains information about a\n",
    "        paper or journal, including author(s), title, publication date, source, and URL.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a request to the ResearchGate website.\n",
    "    url = f\"https://www.researchgate.net/search?q={query}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML response.\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract the data from the HTML.\n",
    "    data = []\n",
    "\n",
    "    # Find all of the search results.\n",
    "    search_results = soup.find_all(\"div\", class_=\"nova-o-stack__item\")\n",
    "\n",
    "    # Iterate over the search results and extract the data.\n",
    "    for search_result in search_results:\n",
    "        try:\n",
    "            title = search_result.find(\"h4\", class_=\"nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-publication-item__title\").text\n",
    "        except AttributeError:\n",
    "            title = \"None\"\n",
    "\n",
    "        try:\n",
    "            author = search_result.find(\"div\", class_=\"nova-v-publication-item__person-list\").text\n",
    "        except AttributeError:\n",
    "            author = \"None\"\n",
    "\n",
    "        try:\n",
    "            source = search_result.find(\"div\", class_=\"nova-v-publication-item__meta-right\").text\n",
    "        except AttributeError:\n",
    "            source = \"None\"\n",
    "\n",
    "        try:\n",
    "            publication_date = search_result.find(\"span\", class_=\"nova-v-publication-item__meta-data-item\").text\n",
    "        except AttributeError:\n",
    "            publication_date = \"None\"\n",
    "\n",
    "        try:\n",
    "            # Get the URL of the paper.\n",
    "            link = search_result.find(\"a\", class_=\"nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare\").get(\"href\")\n",
    "        except AttributeError:\n",
    "            link = \"None\"\n",
    "\n",
    "        # Create a dictionary to store the scraped data.\n",
    "        data.append({\n",
    "            \"Author(s)\": author,\n",
    "            \"Title\": title,\n",
    "            \"Publication Date\": publication_date,\n",
    "            \"Source\": source,\n",
    "            \"URL\": link\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "query = \"heatwave\"\n",
    "results = scrape_researchgate(query)\n",
    "\n",
    "# Save the data to a CSV file.\n",
    "with open(\"researchgate_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"Author(s)\", \"Title\", \"Publication Date\", \"Source\", \"URL\"])\n",
    "    writer.writeheader()\n",
    "\n",
    "    for data in results:\n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d6aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
